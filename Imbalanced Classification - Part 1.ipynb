{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the last milestone of this course. In here, you will use either the text you generated in the previous project, or the ones provided here, to augment training data. In this milestone, the training data consists of 6250 generated positive reviews, 6250 original positive reviews, and 12500 negative reviews. These are concatenated and then shuffled for model training. Steps 1 through 18 are involved with contatenating these data. From step 19 and onwards, the process of building and training the model is identitical to that in project 2, milestone 4, where you built and trained a model using oversampled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Load necessary libraries. Besides the usual TensorFlow, numpy and pandas libraries, you may need pickle library, as sample data provided are serialized in pickle format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Sample files needed for this project are provided for your convenience. These files are:\n",
    "\n",
    "`generated.pickle`: generated texts\\\n",
    "`x_trains_subset_positive_reviews.pickle`: randomly selected positive reviews from training data\\\n",
    "`y_trains_subset_positive_labels.pickle`: positive labels\\\n",
    "`x_train0_negative_reviews.pickle`: all negative reviews from training data\\\n",
    "`y_train0_negative_labels.pickle`: all negative labels from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = pickle.load( open( \"sample_files/generated.pickle\", \"rb\" ) )\n",
    "x_trains = pickle.load( open( \"sample_files/x_trains_subset_positive_reviews.pickle\", \"rb\" ) )\n",
    "y_trains = pickle.load( open( \"sample_files/y_trains_subset_positive_labels.pickle\", \"rb\" ))\n",
    "x_train0 = pickle.load( open( \"sample_files/x_train0_negative_reviews.pickle\", \"rb\" ))\n",
    "y_train0 = pickle.load( open( \"sample_files/y_train0_negative_labels.pickle\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should verify the type of `generated` and take a look at a sample. These are generated, not written by human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6250, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trains.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the same person does a great author roland go about not unthurming in fact from the end of more it's just doing soliday forces her to catch me aher 3 and his purshe doommates sandler's lily richardson who are supposed directed to childhood to bring characterization with a gunaumod on the flock notable are the best comedy that captures to check he played her dandy wears not being a bit more hamr but in her episoning them in fuction desire kept a sleazy sniper who is beautiful and an aflabiane only to make his life for native personality john forlive name school the entire cameo from the drunken a russian norman the black f the decade and she took apparently now what's great here this is always hard to feel like an the that would be inore i've have seen it again and way and we're a good aspect of the crowd it's nronse for yourself why i suppose this as engaging with the way i'm bad for years set at the cinematography by a brave tv series and absurdish floray and fully attached to reconcy on\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Apply squeeze() to x_trains, y_trains, x_train0, y_train0 to get rid of extra dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply squeeze() to x_trains, y_trains, x_train0, y_train0 to get rid of extra dimensions\n",
    "x_trains = x_trains.squeeze()\n",
    "y_trains = y_trains.squeeze()\n",
    "x_train0 = x_train0.squeeze()\n",
    "y_train0 = y_train0.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6250,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trains.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 165,\n",
       " 14,\n",
       " 20,\n",
       " 16,\n",
       " 24,\n",
       " 38,\n",
       " 78,\n",
       " 12,\n",
       " 1367,\n",
       " 206,\n",
       " 212,\n",
       " 5,\n",
       " 2318,\n",
       " 50,\n",
       " 26,\n",
       " 52,\n",
       " 156,\n",
       " 11,\n",
       " 14,\n",
       " 22,\n",
       " 18,\n",
       " 1825,\n",
       " 7517,\n",
       " 39973,\n",
       " 18746,\n",
       " 39,\n",
       " 4,\n",
       " 1419,\n",
       " 3693,\n",
       " 37,\n",
       " 299,\n",
       " 18063,\n",
       " 160,\n",
       " 73,\n",
       " 573,\n",
       " 284,\n",
       " 9,\n",
       " 3546,\n",
       " 4224,\n",
       " 39,\n",
       " 2039,\n",
       " 5,\n",
       " 289,\n",
       " 8822,\n",
       " 4,\n",
       " 293,\n",
       " 105,\n",
       " 26,\n",
       " 256,\n",
       " 34,\n",
       " 3546,\n",
       " 5788,\n",
       " 17,\n",
       " 6184,\n",
       " 37,\n",
       " 16,\n",
       " 184,\n",
       " 52,\n",
       " 5,\n",
       " 82,\n",
       " 163,\n",
       " 21,\n",
       " 4,\n",
       " 31,\n",
       " 37,\n",
       " 91,\n",
       " 770,\n",
       " 72,\n",
       " 16,\n",
       " 628,\n",
       " 8335,\n",
       " 17,\n",
       " 4500,\n",
       " 39520,\n",
       " 29,\n",
       " 299,\n",
       " 6,\n",
       " 275,\n",
       " 109,\n",
       " 74,\n",
       " 29,\n",
       " 633,\n",
       " 127,\n",
       " 88,\n",
       " 11,\n",
       " 85,\n",
       " 108,\n",
       " 40,\n",
       " 4,\n",
       " 1419,\n",
       " 3693,\n",
       " 1395,\n",
       " 5808,\n",
       " 4,\n",
       " 31025,\n",
       " 42,\n",
       " 4,\n",
       " 43737,\n",
       " 29,\n",
       " 299,\n",
       " 6,\n",
       " 55,\n",
       " 2259,\n",
       " 415,\n",
       " 5,\n",
       " 11,\n",
       " 7242,\n",
       " 4,\n",
       " 299,\n",
       " 220,\n",
       " 4,\n",
       " 1961,\n",
       " 6,\n",
       " 132,\n",
       " 209,\n",
       " 101,\n",
       " 1438,\n",
       " 63,\n",
       " 16,\n",
       " 327,\n",
       " 8,\n",
       " 67,\n",
       " 4,\n",
       " 64,\n",
       " 66,\n",
       " 1566,\n",
       " 155,\n",
       " 44,\n",
       " 14,\n",
       " 22,\n",
       " 26,\n",
       " 4,\n",
       " 450,\n",
       " 1268,\n",
       " 7,\n",
       " 4,\n",
       " 182,\n",
       " 3331,\n",
       " 2216,\n",
       " 63,\n",
       " 166,\n",
       " 14,\n",
       " 22,\n",
       " 382,\n",
       " 168,\n",
       " 6,\n",
       " 117,\n",
       " 1967,\n",
       " 444,\n",
       " 13,\n",
       " 197,\n",
       " 14,\n",
       " 16,\n",
       " 6,\n",
       " 184,\n",
       " 52,\n",
       " 117,\n",
       " 22]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trains[:5][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Verify the length of `x_trains`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6250"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_trains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Since generated text surely contains words that are mispelled, it makes sense to use character based tokenization instead of word based tokenization that came with the dataset. The plan here is to encode these reviews at character level. Therefore I need to decode reviews from token to plain text, then merge these text with generated text, then tokenize all text at character level. Remember that in the `tf.keras.datasets.imdb`, the first four integers need to be accounted as reserved tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "# These indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()} \n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 165, 14, 20, 16, 24, 38, 78, 12, 1367, 206, 212, 5, 2318, 50, 26, 52, 156, 11, 14, 22, 18, 1825, 7517, 39973, 18746, 39, 4, 1419, 3693, 37, 299, 18063, 160, 73, 573, 284, 9, 3546, 4224, 39, 2039, 5, 289, 8822, 4, 293, 105, 26, 256, 34, 3546, 5788, 17, 6184, 37, 16, 184, 52, 5, 82, 163, 21, 4, 31, 37, 91, 770, 72, 16, 628, 8335, 17, 4500, 39520, 29, 299, 6, 275, 109, 74, 29, 633, 127, 88, 11, 85, 108, 40, 4, 1419, 3693, 1395, 5808, 4, 31025, 42, 4, 43737, 29, 299, 6, 55, 2259, 415, 5, 11, 7242, 4, 299, 220, 4, 1961, 6, 132, 209, 101, 1438, 63, 16, 327, 8, 67, 4, 64, 66, 1566, 155, 44, 14, 22, 26, 4, 450, 1268, 7, 4, 182, 3331, 2216, 63, 166, 14, 22, 382, 168, 6, 117, 1967, 444, 13, 197, 14, 16, 6, 184, 52, 117, 22]),\n",
       "       list([1, 4, 12475, 9, 6, 680, 22, 94, 293, 109, 5264, 256, 19, 35, 1732, 1493, 7, 1382, 5, 39453, 34, 3977, 9, 24, 129, 801, 632, 5264, 9, 6, 569, 132, 37, 9, 6606, 6, 522, 1696, 113, 18358, 260, 1084, 5284, 153, 11, 4, 5951, 7, 1043, 6448, 588, 29, 150, 659, 309, 11600, 46, 5, 2724, 2610, 5, 38, 103, 6, 580, 2180, 33, 6, 1449, 19, 1738, 5060, 6832, 29, 26797, 23, 5, 778, 6, 27792, 2094, 1862, 1738, 4, 7769, 327, 232, 9, 1951, 19, 49, 538, 11, 27, 205, 113, 5, 882, 30, 579, 100, 361, 6, 464, 17, 73, 4, 107, 97, 35, 2076, 2025, 5, 1738, 3775, 187, 8, 842, 21409, 65, 60, 103, 2799, 4, 13534, 882, 44, 21409, 157, 10, 10, 12475, 6077, 6, 875, 24, 340, 12916, 7, 11, 438, 4, 1210, 632, 5059, 108, 40, 79218, 5, 76289, 15587, 72568, 216, 8, 330, 21, 12475, 12677, 11, 450, 1317, 771, 86, 7, 32, 4, 880, 5, 599, 9, 6270, 21, 115, 66, 617, 11, 101, 1589, 1217, 15, 48, 25, 26, 35, 206, 20, 4301, 267, 18, 35, 10563, 3409, 14, 20, 80, 242, 4363, 25, 5, 333, 1025, 91, 1210, 632, 108, 12475, 166, 57, 589, 8, 123, 10506, 5, 3265, 39, 94, 293, 109, 21409, 292, 9, 331, 1353, 17, 35, 21175, 9, 51, 12, 25671, 243, 7, 155, 14, 9, 1732, 348, 15, 3816, 3816, 7, 178, 62, 1133, 880, 18, 278, 2993, 5, 246, 14, 12894, 1483, 9, 382, 51, 166, 4, 22, 235, 2902, 261, 75, 92, 40, 8, 974, 12, 220, 233, 100, 413, 4885, 103, 75, 122, 12, 196, 195, 279, 60, 588, 122, 21409, 1484, 1833, 8, 783, 37, 9, 2648, 8, 28, 84, 556, 37, 694, 4, 20, 115, 2033, 19, 134, 1204, 4, 1152, 9, 23, 5264, 5, 27, 9129, 12807, 83, 6, 2038, 1862, 48, 25, 332, 44, 294, 40, 5264, 11, 4, 2300, 25, 62, 28, 6, 2581, 197, 15, 84, 40, 90, 144, 30, 3314, 46, 7, 926, 40, 6, 5388, 21, 918, 8, 106, 27, 113, 25, 26, 1309, 11, 34, 27, 1596, 1946, 2506, 18, 4, 2113, 13, 482, 10, 10, 14256, 193, 23, 5264, 9, 73, 224, 5, 1685, 4333, 29, 152, 340, 4313, 309, 39, 27, 592, 1648, 52, 272, 5, 3557, 1382, 21, 247, 43, 1608, 1193, 8828, 83, 4, 1493, 916, 42, 2625, 4613, 11, 4, 655, 9796, 3958, 5, 2117, 7038, 39, 8122, 1382, 8, 3787, 18792, 5, 619, 1680, 16043, 18, 5607, 12, 941, 25, 3099, 44, 27, 4400, 23, 27, 7617, 5, 89, 12, 80, 4525, 148, 187, 90, 45, 6, 227, 40, 2621, 8, 6, 3654, 1799, 15, 2036, 5, 5167, 1936, 6, 355, 854, 137, 29, 299, 21, 12, 495, 4, 108, 64, 85, 678, 217, 15, 7, 1738, 9, 24, 754, 17, 14411, 6832, 505, 11, 6, 1156, 48, 29732, 239, 17, 6, 1988, 914, 19, 6, 1988, 914, 113, 5, 712, 10, 10, 4, 22, 152, 66, 28, 101, 666, 7639, 42, 1983, 314, 27759, 1299, 21, 13, 286, 502, 8, 482, 4, 277, 5, 12, 421, 2349, 12, 152, 28, 101, 933, 4345, 42, 3580, 6102, 5, 246, 12, 421, 55, 406, 5, 12, 161, 28, 101, 483, 11960, 519, 3227, 42, 1056, 3353, 5, 246, 13, 197, 4, 1794, 16, 73, 224, 5, 13, 16, 115, 1100, 279, 4, 64, 147, 749, 133, 9, 44, 4, 406, 359, 8, 2101, 46, 5, 97, 5670, 19, 31, 160, 5, 89, 148, 738, 28, 57, 1515, 42657, 60, 6, 2748, 738, 369, 5, 60, 52, 84, 70, 30, 369, 19, 78, 84, 45, 6, 931, 23, 4, 680, 1526, 182, 75, 412, 11, 6, 52, 22, 290, 319]),\n",
       "       list([1, 13, 104, 14, 20, 69, 8, 30, 253, 8, 97, 12, 18, 178, 12, 16, 253, 8, 106, 12, 4, 156, 168, 40, 36, 28, 6, 253, 58, 61, 7576, 40, 4, 430, 156, 5, 61, 9344, 40, 4, 250, 156, 24, 55, 76, 81, 75, 79, 8, 28, 932, 253, 19, 6, 20, 15, 9, 189, 97, 13, 67, 6, 176, 7, 629, 102, 5, 13, 62, 106, 14, 31, 32, 295, 280, 53, 42, 53, 88, 75, 462, 295, 48, 14, 156, 97, 85, 629, 102, 13, 80, 106, 98, 4, 18468, 1168, 132, 1584, 1313, 8, 516, 4, 156, 9, 55, 76, 6, 52, 78, 132, 29, 97, 178, 462, 295, 4, 91, 13, 62, 202, 14, 20, 6, 312, 603, 48, 25, 942, 72, 10, 10, 13, 92, 124, 48, 4, 2336, 47, 101, 53, 7, 4, 102, 19, 4, 156, 21, 4, 293, 430, 9, 1036, 4, 284, 19, 4, 1758, 4539, 47, 8, 30, 24, 147, 36, 152, 168, 8, 147]),\n",
       "       ...,\n",
       "       list([1, 614, 1028, 796, 522, 3256, 3201, 10, 10, 5077, 127, 31, 67, 38, 111, 478, 906, 109, 156, 220, 1097, 1507, 8, 30, 7364, 11, 31, 22, 60, 151, 6, 171, 64, 977, 18, 3201, 257, 31, 9, 6, 1528, 4, 698, 81, 14, 1493, 7, 212, 5, 147, 113, 6968, 128, 74, 259, 6403, 38, 12, 9, 57, 866, 15, 91, 7, 4, 156, 26, 9341, 10, 10, 4, 228, 9, 87, 57, 824, 76, 69, 8, 30, 2278, 127, 2814, 7078, 66, 297, 4, 9470, 38, 73, 279, 37, 697, 21, 33087, 2946, 16, 21787, 765, 41, 2272, 8904, 481, 131, 505, 61, 13881, 40, 171, 85, 3982, 5171, 10, 10, 17, 35, 3950, 5445, 546, 24, 179, 17, 154, 5, 434, 24, 11, 15, 707, 7, 265, 12, 16, 6, 36003, 22, 17, 73, 6, 87, 22, 18, 259, 625, 1535, 48, 507, 501, 12, 11, 68, 5951, 42, 47562, 845, 12, 203, 30, 5, 7, 265, 6, 87, 5298, 7, 4, 113, 7, 4, 868, 10, 10, 13, 1049, 6, 117, 7, 4, 636, 21, 6, 81885, 790, 158, 5, 12, 434, 166, 75, 181, 8, 67, 4, 226, 155]),\n",
       "       list([1, 14, 9, 61, 514, 7, 4, 922, 827, 1516, 2473, 39, 4, 402, 7239, 4, 204, 310, 19, 20500, 107, 3730, 9, 23, 4, 827, 1516, 7603, 1594, 241, 270, 3815, 31, 5, 8766, 4, 389, 4054, 748, 7, 4, 402, 2473, 4, 1994, 23, 14, 31, 713, 32, 23203, 14982, 405, 21, 53, 1302, 5, 3711, 33, 259, 871, 126, 2674, 56, 522, 149, 629, 102, 42, 1878, 5872, 827, 9, 2621, 8, 6, 948, 1878, 123, 5, 1516, 1068, 8, 297, 6, 612, 7, 3351, 8, 16720, 90, 4, 3020, 456, 3288, 9, 73, 224, 133, 5, 13, 92, 124, 89, 111, 211, 13, 805, 8, 97, 6, 14782, 5, 6, 9688, 15, 629, 54, 13, 16, 6, 554, 54, 11041, 2056, 9, 15019, 34, 4, 26961, 20500, 16, 24, 4, 5094, 12, 16, 41, 313, 59, 214, 31, 2304, 7, 6, 866, 19, 6, 194, 462, 79, 129, 6618, 4558, 1623, 45, 290, 12]),\n",
       "       list([1, 14, 365, 7, 2402, 9, 4796, 11, 800, 5888, 7, 445, 11, 4, 406, 1357, 51, 166, 4, 65, 1739, 9, 6, 2105, 1927, 4957, 109, 37, 34297, 14, 2601, 17, 29, 832, 35, 1356, 430, 143, 4, 63921, 8945, 7478, 83, 4, 957, 7, 27, 205, 3597, 10007, 151, 2402, 2371, 143, 4, 5036, 7, 4, 2694, 7, 341, 29, 3564, 57, 445, 6, 5725, 5, 4005, 26, 50, 8, 5096, 90, 11, 4, 130, 29, 9, 1894, 39, 4, 18678, 7, 1946, 33, 4, 1324, 9, 8593, 4, 91, 3799, 7, 4, 176, 8593, 46776, 64368, 7, 27, 771, 24, 280, 21, 1453, 246, 21081, 88, 29, 9, 8911, 8, 990, 4, 1866, 8593, 9, 433, 74, 988, 21500, 88, 29, 5245, 117, 31176, 37, 413, 7532, 11, 4, 655, 9, 2402, 27, 3093, 9, 40113, 21, 43257, 17, 73, 4, 118, 9, 2406, 1892, 11, 6338, 59, 7477, 988, 61694, 1115, 876, 46, 7, 41, 933, 359, 18, 119, 246, 1025, 8593, 33, 4, 12523, 2322, 7, 41, 205, 113, 59, 127, 24909, 7, 41, 7796, 34, 1907, 2402, 39, 988, 183, 26, 24, 17, 36, 306, 11, 61, 652, 14, 489, 9, 51, 166, 498, 2640, 17998, 13, 62, 202, 14, 22, 6, 158, 21, 94, 14308, 93, 18, 248, 816, 5, 8015, 71, 4075, 444, 6, 478, 12214, 5, 6, 1562, 2316, 65, 18, 32, 1221])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START> actually this movie was not so bad it contains action comedy and excitement there are good actors in this film for instance doug hutchison percy from the green mile who plays bristol another well known actor is jamie kennedy from scream and three kings the main characters are played by jamie foxx as alvin who was pretty good and also funny but the one who most surprised me was david morse as edgar clenteen he plays a different character than he usually does because in other films like the green mile indian runner the negotiator or the langoliers he plays a very sympathetic person and in bait the plays almost the opposite a man without any emotions which was nice to see the only really negative thing about this film are the several pictures of the world trade center which makes this film perhaps look a little dated overall i thought this was a pretty good little film'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_review(x_trains[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Remove `<START>` token. The review is decoded properly as it is coherently written. However, the first token is `<START>`. You need to remove it from each review, because it does not provide any knowledge for text generation; seeing `<START>` doesn't help you predict what next word is. `<START>` is merely a leading label. What you want the model to pay attention to is the text. Since each review is a list of tokens, you may use [`pop`](https://www.geeksforgeeks.org/python-removing-first-element-of-list/) function to remove the first element (token) in each review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove reserved token in subset positive reviews\n",
    "for record in x_trains:\n",
    "  record.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([165, 14, 20, 16, 24, 38, 78, 12, 1367, 206, 212, 5, 2318, 50, 26, 52, 156, 11, 14, 22, 18, 1825, 7517, 39973, 18746, 39, 4, 1419, 3693, 37, 299, 18063, 160, 73, 573, 284, 9, 3546, 4224, 39, 2039, 5, 289, 8822, 4, 293, 105, 26, 256, 34, 3546, 5788, 17, 6184, 37, 16, 184, 52, 5, 82, 163, 21, 4, 31, 37, 91, 770, 72, 16, 628, 8335, 17, 4500, 39520, 29, 299, 6, 275, 109, 74, 29, 633, 127, 88, 11, 85, 108, 40, 4, 1419, 3693, 1395, 5808, 4, 31025, 42, 4, 43737, 29, 299, 6, 55, 2259, 415, 5, 11, 7242, 4, 299, 220, 4, 1961, 6, 132, 209, 101, 1438, 63, 16, 327, 8, 67, 4, 64, 66, 1566, 155, 44, 14, 22, 26, 4, 450, 1268, 7, 4, 182, 3331, 2216, 63, 166, 14, 22, 382, 168, 6, 117, 1967, 444, 13, 197, 14, 16, 6, 184, 52, 117, 22]),\n",
       "       list([4, 12475, 9, 6, 680, 22, 94, 293, 109, 5264, 256, 19, 35, 1732, 1493, 7, 1382, 5, 39453, 34, 3977, 9, 24, 129, 801, 632, 5264, 9, 6, 569, 132, 37, 9, 6606, 6, 522, 1696, 113, 18358, 260, 1084, 5284, 153, 11, 4, 5951, 7, 1043, 6448, 588, 29, 150, 659, 309, 11600, 46, 5, 2724, 2610, 5, 38, 103, 6, 580, 2180, 33, 6, 1449, 19, 1738, 5060, 6832, 29, 26797, 23, 5, 778, 6, 27792, 2094, 1862, 1738, 4, 7769, 327, 232, 9, 1951, 19, 49, 538, 11, 27, 205, 113, 5, 882, 30, 579, 100, 361, 6, 464, 17, 73, 4, 107, 97, 35, 2076, 2025, 5, 1738, 3775, 187, 8, 842, 21409, 65, 60, 103, 2799, 4, 13534, 882, 44, 21409, 157, 10, 10, 12475, 6077, 6, 875, 24, 340, 12916, 7, 11, 438, 4, 1210, 632, 5059, 108, 40, 79218, 5, 76289, 15587, 72568, 216, 8, 330, 21, 12475, 12677, 11, 450, 1317, 771, 86, 7, 32, 4, 880, 5, 599, 9, 6270, 21, 115, 66, 617, 11, 101, 1589, 1217, 15, 48, 25, 26, 35, 206, 20, 4301, 267, 18, 35, 10563, 3409, 14, 20, 80, 242, 4363, 25, 5, 333, 1025, 91, 1210, 632, 108, 12475, 166, 57, 589, 8, 123, 10506, 5, 3265, 39, 94, 293, 109, 21409, 292, 9, 331, 1353, 17, 35, 21175, 9, 51, 12, 25671, 243, 7, 155, 14, 9, 1732, 348, 15, 3816, 3816, 7, 178, 62, 1133, 880, 18, 278, 2993, 5, 246, 14, 12894, 1483, 9, 382, 51, 166, 4, 22, 235, 2902, 261, 75, 92, 40, 8, 974, 12, 220, 233, 100, 413, 4885, 103, 75, 122, 12, 196, 195, 279, 60, 588, 122, 21409, 1484, 1833, 8, 783, 37, 9, 2648, 8, 28, 84, 556, 37, 694, 4, 20, 115, 2033, 19, 134, 1204, 4, 1152, 9, 23, 5264, 5, 27, 9129, 12807, 83, 6, 2038, 1862, 48, 25, 332, 44, 294, 40, 5264, 11, 4, 2300, 25, 62, 28, 6, 2581, 197, 15, 84, 40, 90, 144, 30, 3314, 46, 7, 926, 40, 6, 5388, 21, 918, 8, 106, 27, 113, 25, 26, 1309, 11, 34, 27, 1596, 1946, 2506, 18, 4, 2113, 13, 482, 10, 10, 14256, 193, 23, 5264, 9, 73, 224, 5, 1685, 4333, 29, 152, 340, 4313, 309, 39, 27, 592, 1648, 52, 272, 5, 3557, 1382, 21, 247, 43, 1608, 1193, 8828, 83, 4, 1493, 916, 42, 2625, 4613, 11, 4, 655, 9796, 3958, 5, 2117, 7038, 39, 8122, 1382, 8, 3787, 18792, 5, 619, 1680, 16043, 18, 5607, 12, 941, 25, 3099, 44, 27, 4400, 23, 27, 7617, 5, 89, 12, 80, 4525, 148, 187, 90, 45, 6, 227, 40, 2621, 8, 6, 3654, 1799, 15, 2036, 5, 5167, 1936, 6, 355, 854, 137, 29, 299, 21, 12, 495, 4, 108, 64, 85, 678, 217, 15, 7, 1738, 9, 24, 754, 17, 14411, 6832, 505, 11, 6, 1156, 48, 29732, 239, 17, 6, 1988, 914, 19, 6, 1988, 914, 113, 5, 712, 10, 10, 4, 22, 152, 66, 28, 101, 666, 7639, 42, 1983, 314, 27759, 1299, 21, 13, 286, 502, 8, 482, 4, 277, 5, 12, 421, 2349, 12, 152, 28, 101, 933, 4345, 42, 3580, 6102, 5, 246, 12, 421, 55, 406, 5, 12, 161, 28, 101, 483, 11960, 519, 3227, 42, 1056, 3353, 5, 246, 13, 197, 4, 1794, 16, 73, 224, 5, 13, 16, 115, 1100, 279, 4, 64, 147, 749, 133, 9, 44, 4, 406, 359, 8, 2101, 46, 5, 97, 5670, 19, 31, 160, 5, 89, 148, 738, 28, 57, 1515, 42657, 60, 6, 2748, 738, 369, 5, 60, 52, 84, 70, 30, 369, 19, 78, 84, 45, 6, 931, 23, 4, 680, 1526, 182, 75, 412, 11, 6, 52, 22, 290, 319]),\n",
       "       list([13, 104, 14, 20, 69, 8, 30, 253, 8, 97, 12, 18, 178, 12, 16, 253, 8, 106, 12, 4, 156, 168, 40, 36, 28, 6, 253, 58, 61, 7576, 40, 4, 430, 156, 5, 61, 9344, 40, 4, 250, 156, 24, 55, 76, 81, 75, 79, 8, 28, 932, 253, 19, 6, 20, 15, 9, 189, 97, 13, 67, 6, 176, 7, 629, 102, 5, 13, 62, 106, 14, 31, 32, 295, 280, 53, 42, 53, 88, 75, 462, 295, 48, 14, 156, 97, 85, 629, 102, 13, 80, 106, 98, 4, 18468, 1168, 132, 1584, 1313, 8, 516, 4, 156, 9, 55, 76, 6, 52, 78, 132, 29, 97, 178, 462, 295, 4, 91, 13, 62, 202, 14, 20, 6, 312, 603, 48, 25, 942, 72, 10, 10, 13, 92, 124, 48, 4, 2336, 47, 101, 53, 7, 4, 102, 19, 4, 156, 21, 4, 293, 430, 9, 1036, 4, 284, 19, 4, 1758, 4539, 47, 8, 30, 24, 147, 36, 152, 168, 8, 147]),\n",
       "       ...,\n",
       "       list([614, 1028, 796, 522, 3256, 3201, 10, 10, 5077, 127, 31, 67, 38, 111, 478, 906, 109, 156, 220, 1097, 1507, 8, 30, 7364, 11, 31, 22, 60, 151, 6, 171, 64, 977, 18, 3201, 257, 31, 9, 6, 1528, 4, 698, 81, 14, 1493, 7, 212, 5, 147, 113, 6968, 128, 74, 259, 6403, 38, 12, 9, 57, 866, 15, 91, 7, 4, 156, 26, 9341, 10, 10, 4, 228, 9, 87, 57, 824, 76, 69, 8, 30, 2278, 127, 2814, 7078, 66, 297, 4, 9470, 38, 73, 279, 37, 697, 21, 33087, 2946, 16, 21787, 765, 41, 2272, 8904, 481, 131, 505, 61, 13881, 40, 171, 85, 3982, 5171, 10, 10, 17, 35, 3950, 5445, 546, 24, 179, 17, 154, 5, 434, 24, 11, 15, 707, 7, 265, 12, 16, 6, 36003, 22, 17, 73, 6, 87, 22, 18, 259, 625, 1535, 48, 507, 501, 12, 11, 68, 5951, 42, 47562, 845, 12, 203, 30, 5, 7, 265, 6, 87, 5298, 7, 4, 113, 7, 4, 868, 10, 10, 13, 1049, 6, 117, 7, 4, 636, 21, 6, 81885, 790, 158, 5, 12, 434, 166, 75, 181, 8, 67, 4, 226, 155]),\n",
       "       list([14, 9, 61, 514, 7, 4, 922, 827, 1516, 2473, 39, 4, 402, 7239, 4, 204, 310, 19, 20500, 107, 3730, 9, 23, 4, 827, 1516, 7603, 1594, 241, 270, 3815, 31, 5, 8766, 4, 389, 4054, 748, 7, 4, 402, 2473, 4, 1994, 23, 14, 31, 713, 32, 23203, 14982, 405, 21, 53, 1302, 5, 3711, 33, 259, 871, 126, 2674, 56, 522, 149, 629, 102, 42, 1878, 5872, 827, 9, 2621, 8, 6, 948, 1878, 123, 5, 1516, 1068, 8, 297, 6, 612, 7, 3351, 8, 16720, 90, 4, 3020, 456, 3288, 9, 73, 224, 133, 5, 13, 92, 124, 89, 111, 211, 13, 805, 8, 97, 6, 14782, 5, 6, 9688, 15, 629, 54, 13, 16, 6, 554, 54, 11041, 2056, 9, 15019, 34, 4, 26961, 20500, 16, 24, 4, 5094, 12, 16, 41, 313, 59, 214, 31, 2304, 7, 6, 866, 19, 6, 194, 462, 79, 129, 6618, 4558, 1623, 45, 290, 12]),\n",
       "       list([14, 365, 7, 2402, 9, 4796, 11, 800, 5888, 7, 445, 11, 4, 406, 1357, 51, 166, 4, 65, 1739, 9, 6, 2105, 1927, 4957, 109, 37, 34297, 14, 2601, 17, 29, 832, 35, 1356, 430, 143, 4, 63921, 8945, 7478, 83, 4, 957, 7, 27, 205, 3597, 10007, 151, 2402, 2371, 143, 4, 5036, 7, 4, 2694, 7, 341, 29, 3564, 57, 445, 6, 5725, 5, 4005, 26, 50, 8, 5096, 90, 11, 4, 130, 29, 9, 1894, 39, 4, 18678, 7, 1946, 33, 4, 1324, 9, 8593, 4, 91, 3799, 7, 4, 176, 8593, 46776, 64368, 7, 27, 771, 24, 280, 21, 1453, 246, 21081, 88, 29, 9, 8911, 8, 990, 4, 1866, 8593, 9, 433, 74, 988, 21500, 88, 29, 5245, 117, 31176, 37, 413, 7532, 11, 4, 655, 9, 2402, 27, 3093, 9, 40113, 21, 43257, 17, 73, 4, 118, 9, 2406, 1892, 11, 6338, 59, 7477, 988, 61694, 1115, 876, 46, 7, 41, 933, 359, 18, 119, 246, 1025, 8593, 33, 4, 12523, 2322, 7, 41, 205, 113, 59, 127, 24909, 7, 41, 7796, 34, 1907, 2402, 39, 988, 183, 26, 24, 17, 36, 306, 11, 61, 652, 14, 489, 9, 51, 166, 498, 2640, 17998, 13, 62, 202, 14, 22, 6, 158, 21, 94, 14308, 93, 18, 248, 816, 5, 8015, 71, 4075, 444, 6, 478, 12214, 5, 6, 1562, 2316, 65, 18, 32, 1221])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove reserved token in all negative reviews\n",
    "for record in x_train0:\n",
    "  record.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Now the first token is not 1, which means `<START>` tokens are removed as expected. Next, you are going to put together (by concatenation) positive and negative reviews and convert them to plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_np = np.concatenate((x_trains, x_train0), axis=None) # axis = None is same as axis=0 followed by squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Decode numpy array to plain text, so that you may tokenize these texts. Notice the order of `np_concatenate`  in the previous step. First is the positive subset of original reviews, then all negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_text_holder = []\n",
    "for review in reviews_np:\n",
    "    plain_text = decode_review(review)\n",
    "    plain_text_holder.append(plain_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "10. Inspect text in `plain_text_holder`. Make sure theg text is decoded properly. Simply inspect a few samples to make sure the sentences are coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actually this movie was not so bad it contains action comedy and excitement there are good actors in this film for instance doug hutchison percy from the green mile who plays bristol another well known actor is jamie kennedy from scream and three kings the main characters are played by jamie foxx as alvin who was pretty good and also funny but the one who most surprised me was david morse as edgar clenteen he plays a different character than he usually does because in other films like the green mile indian runner the negotiator or the langoliers he plays a very sympathetic person and in bait the plays almost the opposite a man without any emotions which was nice to see the only really negative thing about this film are the several pictures of the world trade center which makes this film perhaps look a little dated overall i thought this was a pretty good little film']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_text_holder[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18750"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(plain_text_holder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Merge generated text into the training text. The result should be a list. You may use `+` to to combine two lists into one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_text = generated + plain_text_holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the order of `all_training_text`. First is the generated text, then `plain_text_holder`. Therefore, the order of records are: generated positive text, then positive subset of original reviews, then all negative reviews. This order is important, as you have to construct label array to match this order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now original reviews and generated reviews are combined in a list, tokenize this list at character level.\n",
    "type(all_training_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.  Now save `all_training_text` as a pickle file. It will be used as training data in the next milestone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_train_text.pkl', 'wb') as myfile:\n",
    "    pickle.dump(all_training_text, myfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These steps demonstrate how to merge generated text with part of training dataset. The merged text are stored as a pickle file for the next milestone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Their is also aneed to create label for these generated text as positive review. Lets just use the training label already provided by the original data, then concatenate it with labels for positive and negative reviews. Remember the order of `all_training_text`? It is `generated`, followed by the positive, then at last, the negatives. Likewise, your label array needs to follow this order as well. You need to make a label array for generated text. This is a `numpy` array of 1, repeated `len(generated)` times. Then you will concatenate this array, `y_trains`, and `y_train0` together and name it `y_train_assembled`, For `numpy`, it's common practice to save it as a `numpy` file, so you need to save `y_train_assembled` as a numpy file for next milestone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trains_generated = y_trains[:len(generated)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_trains) # Make sure it is the length of generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_assembled = np.array(list(y_trains) + list(y_train0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('y_train_assembled.npy', y_train_assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FYI. This is how you will load a numpy array from a file\n",
    "with open('y_train_assembled.npy', 'rb') as f:\n",
    "    a = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18750"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a) # Verify length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FYI. This is how you will load a pickle file\n",
    "with open('all_train_text.pkl', 'rb') as f:\n",
    "    b = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
