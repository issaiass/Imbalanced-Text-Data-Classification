{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaLuGdfkxSqR"
   },
   "source": [
    "This is the last milestone of this course. In here, you will use either the text you generated in the previous project, or the ones provided here, to augment training data. In this milestone, the training data consists of 6250 generated positive reviews, 6250 original positive reviews, and 12500 negative reviews. These are concatenated and then shuffled for model training. Steps 1 through 18 are involved with contatenating these data. From step 19 and onwards, the process of building and training the model is identitical to that in project 2, milestone 4, where you built and trained a model using oversampled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTGwDZ0ryvoD"
   },
   "source": [
    "## WorkFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0bzgpGrxX_L"
   },
   "source": [
    "1. Download `all_train_text.pkl` and `y_train_assembled.npy` from a GitHub repo provided by author. These files were generated from the mrevious milestone in this project. You may use `git clone` command (pre-installed already in Google Colab) as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X1NsO34Yzi6b",
    "outputId": "aec829ac-99e1-49df-f3ce-892c87e847f1"
   },
   "outputs": [],
   "source": [
    "#%%sh\n",
    "#git clone https://github.com/shinchan75034/ManningPublishing-ImbalancedTextLP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xs9NvnFV3V_e",
    "outputId": "c1d792c2-d6d5-421f-91fc-620c5aa8365d"
   },
   "outputs": [],
   "source": [
    "#!ls -lrt ./ManningPublishing-ImbalancedTextLP/project4/milestone2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnB6Il1tx9k0"
   },
   "source": [
    "With the command above, you can see the two files generated from the previous milestone is now at your disposal, and the directory path to these files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9M5JhZgyR44"
   },
   "source": [
    "2. Load libraries and open these two files. `pkl` file has to be opened with a `pickle` object, while `numpy` file has to be opened with a `numpy` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N59D02ph-2SC",
    "outputId": "f8a3be95-f87f-4133-a803-0dee30d72f19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PKjAVZjA_R35"
   },
   "outputs": [],
   "source": [
    "with open('sample_files/milestone2/all_train_text.pkl', 'rb') as f:\n",
    "    all_training_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YDn7F57w_akE"
   },
   "outputs": [],
   "source": [
    "with open('sample_files/milestone2/y_train_assembled.npy', 'rb') as f:\n",
    "    y_train_assembled = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BRGeDBVy8Tf"
   },
   "source": [
    "3. Since generated text surely contains words that are mispelled, it makes sense to use character based tokenization instead of word based tokenization that came with the dataset. The plan here is to encode these reviews at character level. Therefore I need to decode reviews from token to plain text, then merge these text with generated text, then tokenize all text at character level. Remember that in the `tf.keras.datasets.imdb`, the first four integers need to be accounted as reserved tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IY6oCtCb_yiR",
    "outputId": "55ba61b7-e0a7-4599-997e-936aca8d133e"
   },
   "outputs": [],
   "source": [
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "# These indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()} \n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uY9Sn_kEzOGB"
   },
   "source": [
    "4. Build a new lookup dictionary to map character to index from `all_training_text`. This ensures every character in `all_training_text` are accounted for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lcj7D5NZ_6XN"
   },
   "outputs": [],
   "source": [
    "text = ' '.join(all_training_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Useu4H0v_-Zo",
    "outputId": "d65c9a23-87a5-46aa-bd1a-a9a9975a3b59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ALNvAHXQAFFU"
   },
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eic6DM3zYIm"
   },
   "source": [
    "5. Add padding to the dictionary for marking each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7Yr4q9nVAHf_"
   },
   "outputs": [],
   "source": [
    "# The first indices are reserved\n",
    "char2idx = {k:(v+3) for k,v in char2idx.items()} \n",
    "char2idx[\"<PAD>\"] = 0\n",
    "char2idx[\"<START>\"] = 1\n",
    "char2idx[\"<UNK>\"] = 2  # unknown\n",
    "char2idx[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gW2u5xT4zhsO"
   },
   "source": [
    "6. Convert plain text to tokens. Use `char2idx` to map all text to integers according to `char2idx`. Create a helper function named `encode_review` that encodes a review to token. Then use `map` to apply the helper function to every review. Apply `encode_review` function to every item in `all_training_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XvfnHn_cAJSZ"
   },
   "outputs": [],
   "source": [
    "def encode_review(plain_text):\n",
    "    encoded_list = []\n",
    "    for character in plain_text:\n",
    "        if character in char2idx:\n",
    "            token = char2idx[character]\n",
    "        else:\n",
    "            token = char2idx['<UNK>']\n",
    "        encoded_list.append(token)\n",
    "    return encoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5MBIJDoxALNS"
   },
   "outputs": [],
   "source": [
    "encoded_all_list = list(map(encode_review, all_training_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cDCjzhppANUx",
    "outputId": "d97fd282-3ead-46dc-9520-bd000e48f363"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "if len(encoded_all_list) == len(all_training_text):\n",
    "    print(len(encoded_all_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L90aE7mXzoDs"
   },
   "source": [
    "7. Convert tokenized list to `numpy` array. The tokenized reviews are in Python list format. In order to use it for machine learning model training, you need to convert it to a `numpy` array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9mUtNVyxAO2s",
    "outputId": "b9312e3a-abe7-424e-e52c-c1543fddd198"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Denys\\AppData\\Local\\Temp/ipykernel_1572/3906910307.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  all_training_np = np.array(encoded_all_list)\n"
     ]
    }
   ],
   "source": [
    "all_training_np = np.array(encoded_all_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjW0-qtfz0dn"
   },
   "source": [
    "8. Perform padding operation. \\\n",
    "It turns out that in text classification problem, it is often required to have all data to be in same length. This length totally up to you. You have to look at length of your data and determine what is a reasonable length which contains enough information for a model to learn. In this code, a length of 256 words is set. You may experiment with a different value. For reviews shorter than this length, you will padd it with <padding> token in the front of the text; for reviews longer than this length, you will truncate it at this length. Lets pad each sentence to maximimum length of 256 words. We may take advantage of `pad_sequences` function provided to speed simplify our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lR9DS3UBAZSV"
   },
   "outputs": [],
   "source": [
    "train_data = tf.keras.preprocessing.sequence.pad_sequences(all_training_np,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='pre',\n",
    "                                                        maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IzcczqlDAbhw",
    "outputId": "fd4003d5-ec97-4835-b0e5-98ff490dbe19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 256)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfP3yk8Z0DO9"
   },
   "source": [
    "9. Create a shuffling index. This index is a randomized array of indices. It will be used to shuffle both training data and label, so that training data and label will not mismatch during randomization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iS2K6982AecG"
   },
   "outputs": [],
   "source": [
    "#Create a shuffling index\n",
    "np.random.seed(100) \n",
    "shuffler = np.random.permutation(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "lC90An7CAhvL"
   },
   "outputs": [],
   "source": [
    "#Shuffle both training data and label using the same shuffler\n",
    "train_data_shuffled =  train_data[shuffler] # YOUR WORK: Use shuffler to re-order train_data\n",
    "y_train_assembled_shuffled = y_train_assembled[shuffler] # YOUR WORK: Use shuffler to re-order label (y_train_assembled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5S7YmpJ0iJC"
   },
   "source": [
    "10. Build and compile a text classification model with structure just like in project 2. Start with embedding layer that convert a word into multi-dimensional vector representation. Then we feed that representation to a bidirectional Long-Short Terms Memory cell (LSTM) that uses 128 (a hyperparameter - arbitrarily chosen, feel free to experiment) dimensions to represent text sequence, follow by a dense layer to aggregate the LSTM output before making a classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-q0eEPgbAokX",
    "outputId": "4490915b-9be4-4a27-f402-8a55dacb1020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, None, 256)         22678528  \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 32)                34944     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 22,715,649\n",
      "Trainable params: 22,715,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input shape is the vocabulary count used for the movie reviews (10,000 words)\n",
    "vocab_size = len(word_index)\n",
    "\n",
    "MAX_SENTENCE_LENGTH=256\n",
    "EMBEDDING_SIZE=16\n",
    "HIDDEN_LAYER_SIZE=64\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, output_dim=MAX_SENTENCE_LENGTH),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(EMBEDDING_SIZE)),\n",
    "    tf.keras.layers.Dense(HIDDEN_LAYER_SIZE),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "MjDhlZ3BAqZn"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eQEnNh70zir"
   },
   "source": [
    "11. Set up cross validation. It is a good practice to set up a portion of training data for cross validation at the end of each training epoch. This helps us identify proper training epochs and prevent overfitting by memorizing training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "rCFsM33JAujY"
   },
   "outputs": [],
   "source": [
    "# Shuffle training data for cross validation during training cycle\n",
    "FRAC = 0.8 # fraction of training data used for training. Remaining is for cross validation.\n",
    "idx = np.arange(len(train_data_shuffled))\n",
    "np.random.seed(seed=400)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "idxs = idx[:round(len(idx)*FRAC)] # Select random 80% for training data\n",
    "partial_x_train = train_data_shuffled[idxs]\n",
    "partial_y_train = y_train_assembled_shuffled[idxs]\n",
    "\n",
    "x_val = np.delete(train_data_shuffled, idxs.tolist(), axis=0) # select remaining as cross validation data\n",
    "y_val = np.delete(y_train_assembled_shuffled, idxs.tolist(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "udGb0VICAwle",
    "outputId": "1ca58944-f680-4ecd-f75e-7ad3a7d2e080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 256) (20000,)\n"
     ]
    }
   ],
   "source": [
    "print(partial_x_train.shape, partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjxBlSXa0-nb"
   },
   "source": [
    "As indicated above, you see the training data size. And below is the cross validation data size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W_uWZulKAzKs",
    "outputId": "4c9844b6-4295-4db5-ab6c-729cc0e030b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 256) (5000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEZFVEFV1Lau"
   },
   "source": [
    "12. Add checkpoint and early stopping to training routine. Since there is no guarantee that you will get a best fitted model at the end of training, or an overfitted one, it is important to save the model at each epoch, so you may determine which epoch gives you the best model based on validation accuracy. A way to achieve this goal is to specify model checkpoint configuration. This is done through `tf.keras.callbacks.ModelCheckpoint API`. We can also save the model only if it is the best until now (see `save_best_only` option in this API).\n",
    "\n",
    "Another concept is for stopping the training routine once the model ceasses to improve, say for consecutive `n` epochs. This is done through `tf.keras.callbacks.EarlyStopping` API. We may set it up so that the training routine stop if there are no improvement in validation accuracy after five eppchs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FssmUztiYwBK",
    "outputId": "68326c55-6b1c-4941-a358-683c6ed208db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoint\\ckpt-{epoch}\n"
     ]
    }
   ],
   "source": [
    "# Checkpopint - \n",
    "\n",
    "checkpoint_prefix = os.path.join('./checkpoint', \"ckpt-{epoch}\")\n",
    "print(checkpoint_prefix)\n",
    "\n",
    "myCheckPoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix,\n",
    "    monitor = 'val_accuracy',\n",
    "    mode = 'max',\n",
    "    save_best_only = True\n",
    ")\n",
    "\n",
    "myEarlyStop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'loss',\n",
    "    patience = 5\n",
    ")\n",
    "\n",
    "CALLBACKS = [myCheckPoint, myEarlyStop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpnLwh3c1EEA"
   },
   "source": [
    "13. Launch the training process with training and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nBmDOlQKA1FM",
    "outputId": "171b26e6-9162-43f0-8ec2-48e8143d4266",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1250/1250 [==============================] - 78s 61ms/step - loss: 0.6725 - accuracy: 0.6001 - val_loss: 0.6211 - val_accuracy: 0.6302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_21_layer_call_fn, lstm_cell_21_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40\n",
      "1250/1250 [==============================] - 76s 61ms/step - loss: 0.6173 - accuracy: 0.6398 - val_loss: 0.5926 - val_accuracy: 0.6524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_21_layer_call_fn, lstm_cell_21_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-2\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40\n",
      "1250/1250 [==============================] - 76s 61ms/step - loss: 0.6260 - accuracy: 0.6335 - val_loss: 0.6158 - val_accuracy: 0.6402\n",
      "Epoch 4/40\n",
      "1250/1250 [==============================] - 77s 61ms/step - loss: 0.6242 - accuracy: 0.6424 - val_loss: 0.5975 - val_accuracy: 0.6590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_21_layer_call_fn, lstm_cell_21_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-4\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40\n",
      "1250/1250 [==============================] - 76s 61ms/step - loss: 0.6000 - accuracy: 0.6604 - val_loss: 0.5960 - val_accuracy: 0.6604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_21_layer_call_fn, lstm_cell_21_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-5\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40\n",
      "1250/1250 [==============================] - 76s 61ms/step - loss: 0.6047 - accuracy: 0.6538 - val_loss: 0.6104 - val_accuracy: 0.6342accuracy: 0.65\n",
      "Epoch 7/40\n",
      "1250/1250 [==============================] - 76s 61ms/step - loss: 0.6338 - accuracy: 0.6319 - val_loss: 0.6202 - val_accuracy: 0.6352\n",
      "Epoch 8/40\n",
      "1250/1250 [==============================] - 76s 61ms/step - loss: 0.6118 - accuracy: 0.6457 - val_loss: 0.6045 - val_accuracy: 0.6428\n",
      "Epoch 9/40\n",
      "1250/1250 [==============================] - 76s 61ms/step - loss: 0.5885 - accuracy: 0.6611 - val_loss: 0.5895 - val_accuracy: 0.6626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_21_layer_call_fn, lstm_cell_21_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-9\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-9\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40\n",
      "1250/1250 [==============================] - 76s 61ms/step - loss: 0.5842 - accuracy: 0.6644 - val_loss: 0.5904 - val_accuracy: 0.6646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_21_layer_call_fn, lstm_cell_21_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-10\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-10\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/40\n",
      "1250/1250 [==============================] - 76s 61ms/step - loss: 0.5809 - accuracy: 0.6693 - val_loss: 0.5881 - val_accuracy: 0.6640\n",
      "Epoch 12/40\n",
      "1250/1250 [==============================] - 76s 61ms/step - loss: 0.5723 - accuracy: 0.6776 - val_loss: 0.5783 - val_accuracy: 0.6728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_21_layer_call_fn, lstm_cell_21_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-12\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-12\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40\n",
      "1250/1250 [==============================] - 76s 61ms/step - loss: 0.5886 - accuracy: 0.6676 - val_loss: 0.5799 - val_accuracy: 0.6774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_21_layer_call_fn, lstm_cell_21_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-13\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-13\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/40\n",
      "1250/1250 [==============================] - 77s 61ms/step - loss: 0.5695 - accuracy: 0.6830 - val_loss: 0.5828 - val_accuracy: 0.6750\n",
      "Epoch 15/40\n",
      "1250/1250 [==============================] - 77s 62ms/step - loss: 0.5685 - accuracy: 0.6854 - val_loss: 0.5829 - val_accuracy: 0.6592\n",
      "Epoch 16/40\n",
      "1250/1250 [==============================] - 78s 62ms/step - loss: 0.5688 - accuracy: 0.6849 - val_loss: 0.5859 - val_accuracy: 0.6852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_21_layer_call_fn, lstm_cell_21_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-16\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-16\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/40\n",
      "1250/1250 [==============================] - 78s 63ms/step - loss: 0.5661 - accuracy: 0.6866 - val_loss: 0.5693 - val_accuracy: 0.6870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_21_layer_call_fn, lstm_cell_21_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-17\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-17\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40\n",
      "1250/1250 [==============================] - 77s 62ms/step - loss: 0.5573 - accuracy: 0.6948 - val_loss: 0.5836 - val_accuracy: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_21_layer_call_fn, lstm_cell_21_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-18\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-18\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40\n",
      "1250/1250 [==============================] - 78s 62ms/step - loss: 0.5568 - accuracy: 0.6961 - val_loss: 0.5637 - val_accuracy: 0.7020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_21_layer_call_fn, lstm_cell_21_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-19\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-19\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/40\n",
      "1250/1250 [==============================] - 78s 62ms/step - loss: 0.5465 - accuracy: 0.7092 - val_loss: 0.5667 - val_accuracy: 0.6870\n",
      "Epoch 21/40\n",
      "1250/1250 [==============================] - 77s 62ms/step - loss: 0.5391 - accuracy: 0.7169 - val_loss: 0.5937 - val_accuracy: 0.6734\n",
      "Epoch 22/40\n",
      "1250/1250 [==============================] - 77s 61ms/step - loss: 0.5554 - accuracy: 0.6943 - val_loss: 0.5721 - val_accuracy: 0.6802\n",
      "Epoch 23/40\n",
      "1250/1250 [==============================] - 78s 62ms/step - loss: 0.5545 - accuracy: 0.6942 - val_loss: 0.5562 - val_accuracy: 0.6910\n",
      "Epoch 24/40\n",
      "1250/1250 [==============================] - 77s 62ms/step - loss: 0.5292 - accuracy: 0.7203 - val_loss: 0.5642 - val_accuracy: 0.7096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_21_layer_call_fn, lstm_cell_21_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-24\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-24\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/40\n",
      "1250/1250 [==============================] - 78s 62ms/step - loss: 0.5383 - accuracy: 0.7090 - val_loss: 0.5628 - val_accuracy: 0.7054\n",
      "Epoch 26/40\n",
      "1250/1250 [==============================] - 78s 63ms/step - loss: 0.5338 - accuracy: 0.7172 - val_loss: 0.5607 - val_accuracy: 0.6910\n",
      "Epoch 27/40\n",
      "1250/1250 [==============================] - 78s 62ms/step - loss: 0.5189 - accuracy: 0.7303 - val_loss: 0.5709 - val_accuracy: 0.7286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_21_layer_call_fn, lstm_cell_21_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-27\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-27\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/40\n",
      "1250/1250 [==============================] - 77s 62ms/step - loss: 0.5124 - accuracy: 0.7337 - val_loss: 0.5839 - val_accuracy: 0.6830\n",
      "Epoch 29/40\n",
      "1250/1250 [==============================] - 78s 62ms/step - loss: 0.5371 - accuracy: 0.7140 - val_loss: 0.5656 - val_accuracy: 0.6892\n",
      "Epoch 30/40\n",
      "1250/1250 [==============================] - 78s 62ms/step - loss: 0.5058 - accuracy: 0.7437 - val_loss: 0.5486 - val_accuracy: 0.7082\n",
      "Epoch 31/40\n",
      "1250/1250 [==============================] - 77s 62ms/step - loss: 0.5187 - accuracy: 0.7354 - val_loss: 0.5816 - val_accuracy: 0.6936\n",
      "Epoch 32/40\n",
      "1250/1250 [==============================] - 77s 62ms/step - loss: 0.4989 - accuracy: 0.7455 - val_loss: 0.5476 - val_accuracy: 0.7280\n",
      "Epoch 33/40\n",
      "1250/1250 [==============================] - 77s 62ms/step - loss: 0.4987 - accuracy: 0.7523 - val_loss: 0.5144 - val_accuracy: 0.7434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_20_layer_call_fn, lstm_cell_20_layer_call_and_return_conditional_losses, lstm_cell_21_layer_call_fn, lstm_cell_21_layer_call_and_return_conditional_losses, lstm_cell_20_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-33\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint\\ckpt-33\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/40\n",
      "1250/1250 [==============================] - 77s 62ms/step - loss: 0.4966 - accuracy: 0.7490 - val_loss: 0.5267 - val_accuracy: 0.7398s: 0.4966 - accuracy\n",
      "Epoch 35/40\n",
      "1250/1250 [==============================] - 77s 62ms/step - loss: 0.4755 - accuracy: 0.7696 - val_loss: 0.5783 - val_accuracy: 0.7412\n",
      "Epoch 36/40\n",
      "1250/1250 [==============================] - 79s 63ms/step - loss: 0.4812 - accuracy: 0.7642 - val_loss: 0.5628 - val_accuracy: 0.6950\n",
      "Epoch 37/40\n",
      "1250/1250 [==============================] - 78s 63ms/step - loss: 0.4905 - accuracy: 0.7574 - val_loss: 0.5417 - val_accuracy: 0.7420\n",
      "Epoch 38/40\n",
      "1250/1250 [==============================] - 78s 63ms/step - loss: 0.4611 - accuracy: 0.7728 - val_loss: 0.5299 - val_accuracy: 0.7328\n",
      "Epoch 39/40\n",
      "1250/1250 [==============================] - 78s 62ms/step - loss: 0.4909 - accuracy: 0.7571 - val_loss: 0.5303 - val_accuracy: 0.7358\n",
      "Epoch 40/40\n",
      "1250/1250 [==============================] - 77s 62ms/step - loss: 0.4911 - accuracy: 0.7584 - val_loss: 0.6004 - val_accuracy: 0.6814\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=CALLBACKS,\n",
    "                    verbose=1).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIt2n7E8ffMe"
   },
   "source": [
    "Check the output from the above cell and observe that not every epoch of training results in saving the model weights and biases. Saving these parameters only occurs if the validation acurracy is the best comparing to all previous epochs. Therefore, the last epoch which saves these parameters is the epoch where best model is produced through these training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvJldyte3aKd"
   },
   "source": [
    "14. Load test data and encode it with character-based token. Since the model was trained with character-based tokenized data, in order to test the model with test data, you need to encode the test data with characger-based tokens as well. First you need to decode text data, which is encoded by word based dictionary in the original data, and encode it with the character-based dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WCICTfTsA3a9",
    "outputId": "d29c75f5-4059-4c6a-aab5-2d9d6b482870"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(\n",
    "    path='imdb.npz',\n",
    "    num_words=None,\n",
    "    skip_top=0,\n",
    "    maxlen=None,\n",
    "    seed=113,\n",
    "    start_char=1,\n",
    "    oov_char=2,\n",
    "    index_from=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "2qsYKLYeDEKV"
   },
   "outputs": [],
   "source": [
    "#Remove reserved token in all negative reviews\n",
    "for record in x_test:\n",
    "    record.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "-N9ofKNZDGQi"
   },
   "outputs": [],
   "source": [
    "test_plain_text_holder = []\n",
    "for review in x_test:\n",
    "    plain_text = decode_review(review)\n",
    "    test_plain_text_holder.append(plain_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "JIKbz0eRDH0P"
   },
   "outputs": [],
   "source": [
    "char_encoded_test_list = list(map(encode_review, test_plain_text_holder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCDpfiZQ3nky"
   },
   "source": [
    "15. Preprocessing test data. You need to apply the same preprocessing steps to test data as you did for training data. Namely, the encoded list that holds the test data has to be converted to a `numpy` array, then you need to ensure the maximum length for test data is same as what you set for training, which is specified to be 256 characters. Finally, for test data shorter than the maximum length, you will insert `<PAD>` token in front of the data. After all these processing steps are done, you have a `numpy` array ready for model to score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DYvZtl5hDKyE",
    "outputId": "fb830e7b-c377-4b7a-908c-8ccf520762ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Denys\\AppData\\Local\\Temp/ipykernel_1572/1737193984.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  all_test_np = np.array(char_encoded_test_list)\n"
     ]
    }
   ],
   "source": [
    "all_test_np = np.array(char_encoded_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "Gvb_84C_DOJE"
   },
   "outputs": [],
   "source": [
    "test_data = tf.keras.preprocessing.sequence.pad_sequences(all_test_np,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='pre',\n",
    "                                                       maxlen=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmWJytGh-5j9"
   },
   "source": [
    "16. Find the epoch with best validation accuracy. You may use the code below to extract from training history object about which epoch contains the model with highest validation accuracy. Then you may build up a text string for the file path, and load the model from that epoch using `tf.keras.models.load_model` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LdCDJjjxpg5L",
    "outputId": "f974d614-b03a-436a-8d3f-c65e4dc6a25e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best epoch:  33\n"
     ]
    }
   ],
   "source": [
    "max_value = max(hist['val_accuracy'])\n",
    "max_index = hist['val_accuracy'].index(max_value)\n",
    "best_epoch = max_index + 1\n",
    "print('Best epoch: ', best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "8tWE1ElegMMs"
   },
   "outputs": [],
   "source": [
    "best_trained_model_path = 'checkpoint/ckpt-' + str(best_epoch)\n",
    "my_best_model = tf.keras.models.load_model(best_trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "j9WrO4L2DQZv"
   },
   "outputs": [],
   "source": [
    "predicted = my_best_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bV6HMcxxDSTi",
    "outputId": "61196ea8-b587-429f-f53d-345c3f936f38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.38588992],\n",
       "       [ 0.34800476],\n",
       "       [ 0.2983358 ],\n",
       "       ...,\n",
       "       [ 0.22702295],\n",
       "       [-0.0822449 ],\n",
       "       [ 0.57778704]], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LfcpLt93urF"
   },
   "source": [
    "17. Create a confusion matrix of the prediction. Now you need to make sense of predicted. It is an array of probability. Remember that we decided if the value is greater than 0.5, then the prediction is a positive comment. If the value is less or equal to 0.5, then it is a negative comment. It's a good idea to display the prediction outcome in a confusion table, so we can see the breakout of actual vs. predicted in each class. To do that, you may use pandas library to create a pandas series, then use crosstab function to build a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "5IDrOAQ-DeFS"
   },
   "outputs": [],
   "source": [
    "predicted[predicted > 0.5] = 1\n",
    "predicted[predicted <= 0.5] = 0\n",
    "predictedf = predicted.flatten().astype(int)\n",
    "\n",
    "import pandas as pd\n",
    "df3 = pd.DataFrame(data=predictedf, columns=['predicted'])\n",
    "refdf = pd.DataFrame(data=y_test, columns=['actual'])\n",
    "\n",
    "y_actu = pd.Series(data=refdf.squeeze(), name='ACTUAL')\n",
    "y_pred = pd.Series(data=df3.squeeze(), name='PREDICTED')\n",
    "predicted_results = y_pred.tolist()\n",
    "truth = y_actu.tolist()\n",
    "\n",
    "dl_confusion = pd.crosstab(y_actu, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "-ynGJ1Y3DgFR",
    "outputId": "dd7e116b-83b8-4843-edb0-a1af59687880"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9274</td>\n",
       "      <td>3226</td>\n",
       "      <td>12500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4984</td>\n",
       "      <td>7516</td>\n",
       "      <td>12500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>14258</td>\n",
       "      <td>10742</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted      0      1    All\n",
       "Actual                        \n",
       "0           9274   3226  12500\n",
       "1           4984   7516  12500\n",
       "All        14258  10742  25000"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXqeBrPT33Hk"
   },
   "source": [
    "18. Produce a performance report. You need to produce a report that shows precision, recall, and F1 for each class. A convenient method is using `sklearn`'s `classification_report` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KXx9RFXbDh-Y",
    "outputId": "fd170b67-f7d2-458f-d1fd-05425fa65987"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.74      0.69     12500\n",
      "           1       0.70      0.60      0.65     12500\n",
      "\n",
      "    accuracy                           0.67     25000\n",
      "   macro avg       0.68      0.67      0.67     25000\n",
      "weighted avg       0.68      0.67      0.67     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(refdf, df3)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kapnsm_37HD"
   },
   "source": [
    "The report should indicate that model accuracy is improved over the one you saw at the conclusion of project 2 (the oversampling project), where in that case, you oversampled the positive reviews to make up for data balance. With the result here, it is clear that a model that learned from text can generate new text that bears resemblance to original text. The quality of the generated text is good enough to help train a text classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijw-L3b-Djvw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ManningProject4_Milestone2_FullSolution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
