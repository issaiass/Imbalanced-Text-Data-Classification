{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaLuGdfkxSqR"
   },
   "source": [
    "This is the last milestone of this course. In here, you will use either the text you generated in the previous project, or the ones provided here, to augment training data. In this milestone, the training data consists of 6250 generated positive reviews, 6250 original positive reviews, and 12500 negative reviews. These are concatenated and then shuffled for model training. Steps 1 through 18 are involved with contatenating these data. From step 19 and onwards, the process of building and training the model is identitical to that in project 2, milestone 4, where you built and trained a model using oversampled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTGwDZ0ryvoD"
   },
   "source": [
    "## WorkFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0bzgpGrxX_L"
   },
   "source": [
    "1. Download `all_train_text.pkl` and `y_train_assembled.npy` from a GitHub repo provided by author. These files were generated from the mrevious milestone in this project. You may use `git clone` command (pre-installed already in Google Colab) as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X1NsO34Yzi6b",
    "outputId": "aec829ac-99e1-49df-f3ce-892c87e847f1"
   },
   "outputs": [],
   "source": [
    "#%%sh\n",
    "#git clone https://github.com/shinchan75034/ManningPublishing-ImbalancedTextLP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xs9NvnFV3V_e",
    "outputId": "c1d792c2-d6d5-421f-91fc-620c5aa8365d"
   },
   "outputs": [],
   "source": [
    "#!ls -lrt ./ManningPublishing-ImbalancedTextLP/project4/milestone2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnB6Il1tx9k0"
   },
   "source": [
    "With the command above, you can see the two files generated from the previous milestone is now at your disposal, and the directory path to these files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9M5JhZgyR44"
   },
   "source": [
    "2. Load libraries and open these two files. `pkl` file has to be opened with a `pickle` object, while `numpy` file has to be opened with a `numpy` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N59D02ph-2SC",
    "outputId": "f8a3be95-f87f-4133-a803-0dee30d72f19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PKjAVZjA_R35"
   },
   "outputs": [],
   "source": [
    "with open('sample_files/milestone2/all_train_text.pkl', 'rb') as f:\n",
    "    all_training_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YDn7F57w_akE"
   },
   "outputs": [],
   "source": [
    "with open('sample_files/milestone2/y_train_assembled.npy', 'rb') as f:\n",
    "    y_train_assembled = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BRGeDBVy8Tf"
   },
   "source": [
    "3. Since generated text surely contains words that are mispelled, it makes sense to use character based tokenization instead of word based tokenization that came with the dataset. The plan here is to encode these reviews at character level. Therefore I need to decode reviews from token to plain text, then merge these text with generated text, then tokenize all text at character level. Remember that in the `tf.keras.datasets.imdb`, the first four integers need to be accounted as reserved tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IY6oCtCb_yiR",
    "outputId": "55ba61b7-e0a7-4599-997e-936aca8d133e"
   },
   "outputs": [],
   "source": [
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "# These indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()} \n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uY9Sn_kEzOGB"
   },
   "source": [
    "4. Build a new lookup dictionary to map character to index from `all_training_text`. This ensures every character in `all_training_text` are accounted for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lcj7D5NZ_6XN"
   },
   "outputs": [],
   "source": [
    "text = ' '.join(all_training_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Useu4H0v_-Zo",
    "outputId": "d65c9a23-87a5-46aa-bd1a-a9a9975a3b59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ALNvAHXQAFFU"
   },
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eic6DM3zYIm"
   },
   "source": [
    "5. Add padding to the dictionary for marking each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7Yr4q9nVAHf_"
   },
   "outputs": [],
   "source": [
    "# The first indices are reserved\n",
    "char2idx = {k:(v+3) for k,v in char2idx.items()} \n",
    "char2idx[\"<PAD>\"] = 0\n",
    "char2idx[\"<START>\"] = 1\n",
    "char2idx[\"<UNK>\"] = 2  # unknown\n",
    "char2idx[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gW2u5xT4zhsO"
   },
   "source": [
    "6. Convert plain text to tokens. Use `char2idx` to map all text to integers according to `char2idx`. Create a helper function named `encode_review` that encodes a review to token. Then use `map` to apply the helper function to every review. Apply `encode_review` function to every item in `all_training_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XvfnHn_cAJSZ"
   },
   "outputs": [],
   "source": [
    "def encode_review(plain_text):\n",
    "    encoded_list = []\n",
    "    for character in plain_text:\n",
    "        if character in char2idx:\n",
    "            token = char2idx[character]\n",
    "        else:\n",
    "            token = char2idx['<UNK>']\n",
    "        encoded_list.append(token)\n",
    "    return encoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5MBIJDoxALNS"
   },
   "outputs": [],
   "source": [
    "encoded_all_list = list(map(encode_review, all_training_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cDCjzhppANUx",
    "outputId": "d97fd282-3ead-46dc-9520-bd000e48f363"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "if len(encoded_all_list) == len(all_training_text):\n",
    "    print(len(encoded_all_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L90aE7mXzoDs"
   },
   "source": [
    "7. Convert tokenized list to `numpy` array. The tokenized reviews are in Python list format. In order to use it for machine learning model training, you need to convert it to a `numpy` array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9mUtNVyxAO2s",
    "outputId": "b9312e3a-abe7-424e-e52c-c1543fddd198"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Denys\\AppData\\Local\\Temp/ipykernel_1572/3906910307.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  all_training_np = np.array(encoded_all_list)\n"
     ]
    }
   ],
   "source": [
    "all_training_np = np.array(encoded_all_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjW0-qtfz0dn"
   },
   "source": [
    "8. Perform padding operation. \\\n",
    "It turns out that in text classification problem, it is often required to have all data to be in same length. This length totally up to you. You have to look at length of your data and determine what is a reasonable length which contains enough information for a model to learn. In this code, a length of 256 words is set. You may experiment with a different value. For reviews shorter than this length, you will padd it with <padding> token in the front of the text; for reviews longer than this length, you will truncate it at this length. Lets pad each sentence to maximimum length of 256 words. We may take advantage of `pad_sequences` function provided to speed simplify our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "lR9DS3UBAZSV"
   },
   "outputs": [],
   "source": [
    "train_data = tf.keras.preprocessing.sequence.pad_sequences(all_training_np,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='pre',\n",
    "                                                        maxlen=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IzcczqlDAbhw",
    "outputId": "fd4003d5-ec97-4835-b0e5-98ff490dbe19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 256)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfP3yk8Z0DO9"
   },
   "source": [
    "9. Create a shuffling index. This index is a randomized array of indices. It will be used to shuffle both training data and label, so that training data and label will not mismatch during randomization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iS2K6982AecG"
   },
   "outputs": [],
   "source": [
    "#Create a shuffling index\n",
    "np.random.seed(100) \n",
    "shuffler = np.random.permutation(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "lC90An7CAhvL"
   },
   "outputs": [],
   "source": [
    "#Shuffle both training data and label using the same shuffler\n",
    "train_data_shuffled =  train_data[shuffler] # YOUR WORK: Use shuffler to re-order train_data\n",
    "y_train_assembled_shuffled = y_train_assembled[shuffler] # YOUR WORK: Use shuffler to re-order label (y_train_assembled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5S7YmpJ0iJC"
   },
   "source": [
    "10. Build and compile a text classification model with structure just like in project 2. Start with embedding layer that convert a word into multi-dimensional vector representation. Then we feed that representation to a bidirectional Long-Short Terms Memory cell (LSTM) that uses 128 (a hyperparameter - arbitrarily chosen, feel free to experiment) dimensions to represent text sequence, follow by a dense layer to aggregate the LSTM output before making a classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-q0eEPgbAokX",
    "outputId": "4490915b-9be4-4a27-f402-8a55dacb1020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 88588)       5669632   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               90846208  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 96,532,353\n",
      "Trainable params: 96,532,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input shape is the vocabulary count used for the movie reviews (10,000 words)\n",
    "vocab_size = len(word_index)\n",
    "\n",
    "MAX_SENTENCE_LENGTH=256\n",
    "EMBEDDING_SIZE=16\n",
    "HIDDEN_LAYER_SIZE=64\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=64, output_dim=vocab_size),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(MAX_SENTENCE_LENGTH//2)),\n",
    "    tf.keras.layers.Dense(HIDDEN_LAYER_SIZE),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "MjDhlZ3BAqZn"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eQEnNh70zir"
   },
   "source": [
    "11. Set up cross validation. It is a good practice to set up a portion of training data for cross validation at the end of each training epoch. This helps us identify proper training epochs and prevent overfitting by memorizing training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "rCFsM33JAujY"
   },
   "outputs": [],
   "source": [
    "# Shuffle training data for cross validation during training cycle\n",
    "FRAC = 0.8 # fraction of training data used for training. Remaining is for cross validation.\n",
    "idx = np.arange(len(train_data_shuffled))\n",
    "np.random.seed(seed=400)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "idxs = idx[:round(len(idx)*FRAC)] # Select random 80% for training data\n",
    "partial_x_train = train_data_shuffled[idxs]\n",
    "partial_y_train = y_train_assembled_shuffled[idxs]\n",
    "\n",
    "x_val = np.delete(train_data_shuffled, idxs.tolist(), axis=0) # select remaining as cross validation data\n",
    "y_val = np.delete(y_train_assembled_shuffled, idxs.tolist(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "udGb0VICAwle",
    "outputId": "1ca58944-f680-4ecd-f75e-7ad3a7d2e080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 256) (20000,)\n"
     ]
    }
   ],
   "source": [
    "print(partial_x_train.shape, partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjxBlSXa0-nb"
   },
   "source": [
    "As indicated above, you see the training data size. And below is the cross validation data size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W_uWZulKAzKs",
    "outputId": "4c9844b6-4295-4db5-ab6c-729cc0e030b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 256) (5000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEZFVEFV1Lau"
   },
   "source": [
    "12. Add checkpoint and early stopping to training routine. Since there is no guarantee that you will get a best fitted model at the end of training, or an overfitted one, it is important to save the model at each epoch, so you may determine which epoch gives you the best model based on validation accuracy. A way to achieve this goal is to specify model checkpoint configuration. This is done through `tf.keras.callbacks.ModelCheckpoint API`. We can also save the model only if it is the best until now (see `save_best_only` option in this API).\n",
    "\n",
    "Another concept is for stopping the training routine once the model ceasses to improve, say for consecutive `n` epochs. This is done through `tf.keras.callbacks.EarlyStopping` API. We may set it up so that the training routine stop if there are no improvement in validation accuracy after five eppchs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FssmUztiYwBK",
    "outputId": "68326c55-6b1c-4941-a358-683c6ed208db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoint\\ckpt-{epoch}\n"
     ]
    }
   ],
   "source": [
    "# Checkpopint - \n",
    "\n",
    "checkpoint_prefix = os.path.join('./checkpoint', \"ckpt-{epoch}\")\n",
    "print(checkpoint_prefix)\n",
    "\n",
    "myCheckPoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix,\n",
    "    monitor = 'val_accuracy',\n",
    "    mode = 'max',\n",
    "    save_best_only = True\n",
    ")\n",
    "\n",
    "myEarlyStop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'loss',\n",
    "    patience = 5\n",
    ")\n",
    "\n",
    "CALLBACKS = [myCheckPoint, myEarlyStop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpnLwh3c1EEA"
   },
   "source": [
    "13. Launch the training process with training and validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n",
    "session = tf.compat.v1.InteractiveSession(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nBmDOlQKA1FM",
    "outputId": "171b26e6-9162-43f0-8ec2-48e8143d4266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "   OOM when allocating tensor with shape[88588,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node split}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[sequential/bidirectional/backward_lstm/PartitionedCall]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_4914]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1572/978052777.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m hist = model.fit(partial_x_train,\n\u001b[0m\u001b[0;32m      2\u001b[0m                     \u001b[0mpartial_y_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sdcnd\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sdcnd\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sdcnd\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sdcnd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sdcnd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sdcnd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\sdcnd\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:    OOM when allocating tensor with shape[88588,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node split}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[sequential/bidirectional/backward_lstm/PartitionedCall]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_4914]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=CALLBACKS,\n",
    "                    verbose=1).history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIt2n7E8ffMe"
   },
   "source": [
    "Check the output from the above cell and observe that not every epoch of training results in saving the model weights and biases. Saving these parameters only occurs if the validation acurracy is the best comparing to all previous epochs. Therefore, the last epoch which saves these parameters is the epoch where best model is produced through these training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvJldyte3aKd"
   },
   "source": [
    "14. Load test data and encode it with character-based token. Since the model was trained with character-based tokenized data, in order to test the model with test data, you need to encode the test data with characger-based tokens as well. First you need to decode text data, which is encoded by word based dictionary in the original data, and encode it with the character-based dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WCICTfTsA3a9",
    "outputId": "d29c75f5-4059-4c6a-aab5-2d9d6b482870"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(\n",
    "    path='imdb.npz',\n",
    "    num_words=None,\n",
    "    skip_top=0,\n",
    "    maxlen=None,\n",
    "    seed=113,\n",
    "    start_char=1,\n",
    "    oov_char=2,\n",
    "    index_from=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qsYKLYeDEKV"
   },
   "outputs": [],
   "source": [
    "#Remove reserved token in all negative reviews\n",
    "for record in x_test:\n",
    "  record.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-N9ofKNZDGQi"
   },
   "outputs": [],
   "source": [
    "test_plain_text_holder = []\n",
    "for review in x_test:\n",
    "    plain_text = decode_review(review)\n",
    "    test_plain_text_holder.append(plain_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIKbz0eRDH0P"
   },
   "outputs": [],
   "source": [
    "char_encoded_test_list = list(map(encode_review, test_plain_text_holder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCDpfiZQ3nky"
   },
   "source": [
    "15. Preprocessing test data. You need to apply the same preprocessing steps to test data as you did for training data. Namely, the encoded list that holds the test data has to be converted to a `numpy` array, then you need to ensure the maximum length for test data is same as what you set for training, which is specified to be 256 characters. Finally, for test data shorter than the maximum length, you will insert `<PAD>` token in front of the data. After all these processing steps are done, you have a `numpy` array ready for model to score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DYvZtl5hDKyE",
    "outputId": "fb830e7b-c377-4b7a-908c-8ccf520762ca"
   },
   "outputs": [],
   "source": [
    "all_test_np = np.array(char_encoded_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gvb_84C_DOJE"
   },
   "outputs": [],
   "source": [
    "test_data = tf.keras.preprocessing.sequence.pad_sequences(all_test_np,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='pre',\n",
    "                                                       maxlen=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmWJytGh-5j9"
   },
   "source": [
    "16. Find the epoch with best validation accuracy. You may use the code below to extract from training history object about which epoch contains the model with highest validation accuracy. Then you may build up a text string for the file path, and load the model from that epoch using `tf.keras.models.load_model` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LdCDJjjxpg5L",
    "outputId": "f974d614-b03a-436a-8d3f-c65e4dc6a25e"
   },
   "outputs": [],
   "source": [
    "max_value = max(hist['val_accuracy'])\n",
    "max_index = hist['val_accuracy'].index(max_value)\n",
    "best_epoch = max_index + 1\n",
    "print('Best epoch: ', best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tWE1ElegMMs"
   },
   "outputs": [],
   "source": [
    "best_trained_model_path = './text_classifier/ckpt-' + str(best_epoch)\n",
    "my_best_model = tf.keras.models.load_model(best_trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9WrO4L2DQZv"
   },
   "outputs": [],
   "source": [
    "predicted = my_best_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bV6HMcxxDSTi",
    "outputId": "61196ea8-b587-429f-f53d-345c3f936f38"
   },
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LfcpLt93urF"
   },
   "source": [
    "17. Create a confusion matrix of the prediction. Now you need to make sense of predicted. It is an array of probability. Remember that we decided if the value is greater than 0.5, then the prediction is a positive comment. If the value is less or equal to 0.5, then it is a negative comment. It's a good idea to display the prediction outcome in a confusion table, so we can see the breakout of actual vs. predicted in each class. To do that, you may use pandas library to create a pandas series, then use crosstab function to build a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IDrOAQ-DeFS"
   },
   "outputs": [],
   "source": [
    "predicted[predicted > 0.5] = 1\n",
    "predicted[predicted <= 0.5] = 0\n",
    "predictedf = predicted.flatten().astype(int)\n",
    "\n",
    "import pandas as pd\n",
    "df3 = pd.DataFrame(data=predictedf, columns=['predicted'])\n",
    "refdf = pd.DataFrame(data=y_test, columns=['actual'])\n",
    "\n",
    "y_actu = pd.Series(# YOUR WORK HERE, name='ACTUAL')\n",
    "y_pred = pd.Series(# YOUR WORK HERE, name='PREDICTED')\n",
    "predicted_results = y_pred.tolist()\n",
    "truth = y_actu.tolist()\n",
    "\n",
    "dl_confusion = pd.crosstab(y_actu, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "-ynGJ1Y3DgFR",
    "outputId": "dd7e116b-83b8-4843-edb0-a1af59687880"
   },
   "outputs": [],
   "source": [
    "dl_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXqeBrPT33Hk"
   },
   "source": [
    "18. Produce a performance report. You need to produce a report that shows precision, recall, and F1 for each class. A convenient method is using `sklearn`'s `classification_report` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KXx9RFXbDh-Y",
    "outputId": "fd170b67-f7d2-458f-d1fd-05425fa65987"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(# YOUR WORK HERE)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kapnsm_37HD"
   },
   "source": [
    "The report should indicate that model accuracy is improved over the one you saw at the conclusion of project 2 (the oversampling project), where in that case, you oversampled the positive reviews to make up for data balance. With the result here, it is clear that a model that learned from text can generate new text that bears resemblance to original text. The quality of the generated text is good enough to help train a text classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijw-L3b-Djvw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ManningProject4_Milestone2_FullSolution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
